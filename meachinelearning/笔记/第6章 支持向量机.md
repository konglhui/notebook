# 第6章 支持向量机

## 6.1 间隔与支持向量

给定训练样本集，分类学习最基本的想法就是基于训练集D在样本空间中找出一个划分超平面，将不同类别的样本分开，但能将训练样本分来的划分超平面有很多，我们应该努力去找到哪个那。

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180516/25CLfbldmk.png)

这个划分超平面产生的分类结果是最鲁棒的，对未见实例的泛化能力最强。

在样本空间中，划分超平面可通过如下线性方程来描述：

$$
w^{T}x + b = 0 				//w为法向量，b为位移项
$$

样本空间任意一点x到超平面的距离为：
$$
r = \frac{|w^{T}x + b|}{||w||}
$$
假设超平面（w，b）能将训练样本正确分类，即对（x，y） = D，若y = +1，则有$$w^{T}x + b>0$$;若y = -1，则有$$w^{T}x + b<0$$。令
$$
w^{T}x + b>+1,y = +1
$$

$$
w^{T}x + b<-1,y = -1
$$

如图所示：

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180516/fd64Ha5dHI.png)

距离超平面最近的这几个训练样本点使式（6.3）的等号成立，它们被称为“支持向量”，两个一类支持向量到超平买你的距离之和为
$$
r = \frac{2}{||w||}
$$
它称为“间隔”。

要找到具有最大间隔的划分超平面，也就是要找到能满足上述式子中约束的参数w和b，使的间隔最大。即：
$$
max_{w,b} \frac{2}{||w||}
$$

$$
s.t. y(w^{T}x + b) >=1,对于所有的x，y都满足此式子。
$$

上述式子等价于：
$$
max_{w,b} \frac{1}{2}||w||^{2}
$$

$$
s.t. y(w^{T}x + b) >=1,对于所有的x，y都满足此式子。
$$

## 6.2 对偶问题

对上面的式子添加拉格朗日乘子$$a_{i}>=0$$，则该问题可写为：

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180516/b0c7Dl69j9.png)

通过式子对w，b进行求导，（由于a>=0,约束也是大于零）所以能得到，在（w，b）求导后为0时，能得到拉格朗日式子的最小值。也就满足了条件。

同时，将w，b带回到原来的式子，也就能得到对偶问题（就是将问题转化成了求拉格朗日式子的最小值问题）。

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180516/mDbDg0eAIc.png)

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180516/JfglFk2EDb.png)

求解方法常见的有SMP是其中一个著名的代表。

## 6.3 核函数

对于在原本样本空间中并不存在一个能正确划分两类样本的超平面的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180516/0LkmmIijhi.png)

即xi与xj在特征空间的内积等于他们在原始样本空间通过函数k（·，·）计算的结果。

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180516/eG0Kl0lb1L.png)

**对于什么样的函数能做核函数？我们有瞎买你的定理**

**核函数定理：**

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180516/D05dg6Bf31.png)

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180516/14eGIG2JB2.png)

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180516/lFEc3lACj7.png)

## 6.4 软间隔与正则化

在勤勉的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。然而在显示任务中，往往很难确定合适的核函数使得训练样本在特征空间中线性可分。

为此，要引入”软间隔“的概念。

软间隔是某些样本不满足约束：

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180517/HJA0IcGlgb.png)

对于$l_{0/1}$我们给出了三种替代的损失函数：

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180522/liFDdmlIbF.png)

若采用hinge损失，则式子就会变成：

$$
min_{w,b}  \frac{1}{2}||w||^{2}+ C \sum(0,1-y_{i}(w^{T}x_{i}+b)
$$

引入松弛变量，$\zeta_{i}>=0$,可以将式子重写为：

$$
min_{w,b}  \frac{1}{2}||w||^{2}+ C \sum_{i=1}^{m} \zeta_{i}
$$

$$
s.t.y_{i}(w^{T}x_{i}+b)>=1-\zeta_{i}
$$

$$
\zeta_{i}>=0,i = 1,2,...m
$$



这就是软间隔支持向量机。

上述式子进行拉格朗日乘数法可得：

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180522/eFkk9Hfgc9.png)

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180522/hgKa2bF28d.png)

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180526/h2el6h2c85.png)

## 6.5 支持向量回归

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180526/eIAhAc5he0.png)









