{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 泰坦尼克生存预测\n",
    "## 项目流程\n",
    "**这个竞赛的解决方案分七步**：\n",
    "- 1.问题的定义\n",
    "\n",
    "- 2.获取训练集和测试集\n",
    "\n",
    "- 3.准备数据和清洗数据\n",
    "\n",
    "- 4.分析、识别并探索数据\n",
    "\n",
    "- 5.建立模型去预测并解决问题\n",
    "\n",
    "- 6.数据可视化并提出问题解决步骤和最终解决方案\n",
    "\n",
    "- 7.提交结果\n",
    "\n",
    "工作流程是指工作事项的活动流向顺序。但是也有例外的用例。\n",
    "\n",
    "- 我们有时会将几个流程组合在一起，可能会同过可视化分析数据。\n",
    "\n",
    "- 提前完成一个流程，在清洗数据前后我们都会分析数据。\n",
    "\n",
    "- 在工作中会多次执行一个流程，多次使用可视化。\n",
    "\n",
    "- 放弃一个步骤，在本次竞赛中可能不需要某些步骤。\n",
    "\n",
    "## 问题的定义\n",
    "\n",
    "想kaggle这样的竞赛网站可以定义要解决的问题或提出问题，同时提供用于同于训练模型的数据集。在kaggle中泰坦尼克号生还率的竞赛做了描述：https://www.kaggle.com/c/titanic\n",
    "\n",
    "> 列出泰坦尼克号灾难中幸存或未存活的乘客的样本训练集可知，如果测试数据集中的这些乘客存活与否，我们的模型是否可以在给定的测试数据集来确定，而测试数据集不包含存活信息。\n",
    "\n",
    "这下面是kaggle比赛页面描述的几点要点。\n",
    "\n",
    "- 在1912年4月15日是它的初次航行，泰坦尼克号撞上冰山后沉没，在2224名乘客和机组人员有1502名丧生，存活率仅为32%。\n",
    "\n",
    "- 这其中最主要的原因就是，没有足够的救生舱以供使用。\n",
    "\n",
    "- 尽管在沉船过程中有运气的成分存在，像孩子、女人和上层人士的存活率比其他人高。\n",
    "\n",
    "## 工作流程的目标\n",
    "\n",
    "数据科学的工作流程主要有这七个目标。\n",
    "\n",
    "**分类**：把我们的例子进行分类，我们要了解不同类与我们要解决的问题的相关性。\n",
    "\n",
    "**相关性**：可以根据训练集的可用特征来处理这个问题，训练集内的那些特征对我们要解决的问题是重要的？从统计学的角度来说，一个特征与要解决的问题有哪些关联？一个特征值改变了那解决方案也会变吗？这个可以再给定的数据集中的数字特征和种类特征进行测试。我们也会去判断除生存特征之外的特征之间的相关性。某些特征之间的相关性会帮助我们创造、完成或修正特征。\n",
    "\n",
    "**转化**：在建模阶段，需要准备数据。对于我们选择的算法可能需要我们将所有的数据都转化为数字特征。例如将文本特征值转化为数字特征值。\n",
    "\n",
    "**填充**：数据准备阶段也需要我们来填充特征的缺失值，在没有缺失值时模型算法结果会更准确。\n",
    "\n",
    "**校正**：在给定的训练集中有一些是错误的特征值，我们要尝试将这些值找出来修正他们或将他们排除在外。一种方法是检测样本的异常值。如果这个特征对我们的分析或结果不产生影响，我们可以丢弃这个特征。\n",
    "\n",
    "**创造**：基于已经存在的一个或多个特征创造一个新的特征，这样新的特征就会有相关性和完整性。\n",
    "\n",
    "**制图**：在给定的数据和需要解决的问题中怎样选择正确的可视化图形。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据分析和整理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "#可视化\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#机器学习\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据\n",
    "\n",
    "python的pandas包能帮助我们获取数据，我们将数据集放入pandas的DataFrame中。并将两个数据集组合到一起，一起来执行某些操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/Users/Administrator/data/train.csv')\n",
    "test_df = pd.read_csv('C:/Users/Administrator/data/test.csv')\n",
    "combine = [train_df,test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析数据\n",
    "\n",
    "在项目早期，pandas通过描述帮助我们解决以下几个问题。\n",
    "\n",
    "#### 在数据中有那些特征？\n",
    "\n",
    "直接操作或分析这些特性的特性名称，这些特征名在kaggle页面有描述。https://www.kaggle.com/c/titanic/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 哪些是类别特征？\n",
    "\n",
    "这些值将样本分为几组相似的样本。类别特征中的值是名义值，序数值，比值还是区间值？除此之外，这还有助于我们选择合适的图表进行可视化。\n",
    "\n",
    "- 类别值：Survived，Sex and Embarked \n",
    "\n",
    "- 序数值：Pclass\n",
    "\n",
    "#### 哪些是数字特征？\n",
    "\n",
    "哪些是数字特征？这些值随着样本的不同而不同。数值特征有连续的，离散的，还有以时间序列为基础的。除此之外，这还有助于我们选择合适的图表进行可视化。\n",
    "\n",
    "- 连续值：Age,Fare.\n",
    "\n",
    "- 离散值：SibSp,Parch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 哪些特征是混合数字类型？\n",
    "\n",
    "一些特征是数字或alpha数字数据。\n",
    "\n",
    "- Ticket是一种数字和alpha数字的混合数据类型。\n",
    "\n",
    "- Cabin是alpha数字数据类型。\n",
    "\n",
    "#### 哪些特征可能会包含错误或拼写错误？\n",
    "\n",
    "在一个很大的数据集中我们很难检查错误，可是如果我们只检查几个简单的数据集可能会告诉我们哪些特征需要校正。\n",
    "\n",
    "- 名称功能可能包含错误或拼写错误，因为有多种方式可用于描述名称，包括标题，圆括号以及用于替代或短名称的引号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 哪些特征包含空白值？\n",
    "\n",
    "这些空白值是需要修正的。\n",
    "\n",
    "- Cabin>Age>Embarked 这些特征在训练集上包含着一些空值。\n",
    "\n",
    "- Cabin>Age 在测试数据集上也是不完全的。\n",
    "\n",
    "#### 各种特征都是什么数据类型？\n",
    "\n",
    "这个信息在转换目标时会帮助我们。\n",
    "\n",
    "- 七个特征是整数或浮点值，在数据集上是六个。\n",
    "\n",
    "- 五个是字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "print('-'*40)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在整个样本中，数字特征值是怎样分布的？\n",
    "\n",
    "这有助于我们确定在其他早期见解中，代表实际问题领域的训练数据集的代表性。\n",
    "\n",
    "- 样本数是891，是真实数据（2224）的40%。\n",
    "\n",
    "- Survived是一个类别特征，特征值是0或1。\n",
    "\n",
    "- 训练集的生还率是38%，真实生还率为32%、\n",
    "\n",
    "- 大约有75%的乘客是没有父母或孩子。\n",
    "\n",
    "- 大约有30%的乘客室友兄弟姐妹或配偶的。\n",
    "\n",
    "- 仅有1%的乘客的票价是高于512的。\n",
    "\n",
    "- 仅有1%的乘客的年龄是在65-80之间的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 类别特征值是怎样分布的？\n",
    "\n",
    "- Name在特种值中是独一无二的。\n",
    "\n",
    "- Sex只有两个可能值。65%是男性。\n",
    "\n",
    "- Cabin在样本中有一些是重复的，有一些乘客共用同一个小木屋。\n",
    "\n",
    "- Embarked有三个可能值，S上车点是乘客最多的地点。\n",
    "\n",
    "- Ticket特征有高重复率为22%.(有681个票价是唯一的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe(include = ['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对分析的数据进行假设\n",
    "\n",
    "根据数据分析，我们得出了以下假设。在采取适当行动之前，我们可以进一步验证这些假设。\n",
    "\n",
    "**相关性**\n",
    "\n",
    "我们想知道每一个特征与Survived的相关性。我们要在项目的早期就做到这一点，并在项目后期将它与模型化关联进行匹配。\n",
    "\n",
    "**补全**\n",
    "\n",
    "- 1.我们要补全Age特征，因为它与是否生还息息相关。\n",
    "\n",
    "- 2.我们也要补全Embarked特征，因为它也可能与Survived或其他重要特征有关。\n",
    "\n",
    "**校正**\n",
    "\n",
    "- 1.票价特征可能会被删除，因为他有22%的值是相同的，并且这很有可能与survived没有什么关系。\n",
    "\n",
    "- 2.Cabin特征也可能会被删除，因为他在训练集和测试集中有很多空值。\n",
    "\n",
    "- 3.PassenerId也应该从训练集中删除，因为它与Survived没什么关系。\n",
    "\n",
    "- 4.Name特征是很不标准的，它可能与Survivied没什么关系，所以可能被删除。\n",
    "\n",
    "**创造**\n",
    "\n",
    "- 1.我们现在要创造一个新的特征，Family，它是基于SibSp和Parch得到的关于家庭人数的总和。\n",
    "\n",
    "- 2.我们想要从Name特征中提出一些与要求有关的内容来作为新的特征。\n",
    "\n",
    "- 3.我们把年龄分层来创建一个新的特征，这会将连续的数字特征变为有序的数字特征。\n",
    "\n",
    "- 4.如果把票价分层能对我们分析数据有益处，那就以它来创建一个新的特征。\n",
    "\n",
    "**分类**\n",
    "\n",
    "我们还可以根据前面的问题描述添加我们的假设。\n",
    "\n",
    "- 1.女人的生还率可能会更高。\n",
    "\n",
    "- 2.孩子的生还率可能会更高\n",
    "\n",
    "- 3.上册人士的生还率可能会更高。\n",
    "\n",
    "#### 通过列表来进行分析\n",
    "\n",
    "为了确认一些观点和假设，我们快速的分析一些特征之间的相关性。在这个阶段，我们只能在没有空值的特征上进行。这样做只对类别特征类型、序列特征类型、离散特征类型有作用。\n",
    "\n",
    "- **Pclass** 我们能看到它与Survived的相关性很大（>0.5），我们决定在我们的模型中保留这个特征。\n",
    "\n",
    "- **Sex** 我们能看到女性的生存率达到了74%。\n",
    "\n",
    "- **SibSp和Parch** 这个特征没有相关性，最好来创建一个特征或者一组特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by = 'Survived',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['Sex','Survived']].groupby(['Sex'],as_index = False).mean().sort_values(by = 'Survived',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['SibSp','Survived']].groupby(['SibSp'],as_index = False).mean().sort_values(by = 'Survived',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['Parch','Survived']].groupby(['Parch'],as_index = False).mean().sort_values(by = 'Survived',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过可视化来分析数据\n",
    "\n",
    "现在我们使用可视化数据继续确认我们的假设。\n",
    "\n",
    "#### 数字特征的相关性\n",
    "\n",
    "可视化会让我们理解连续数字特征与Survived之间的关联性。\n",
    "\n",
    "对于像年龄这样的连续性数字特征使用直方图是很有效的，这有助于我们队数值进行分层。这个直方图也能自动的对例子进行区分。这对我们分析数据是非常有帮助的。\n",
    "\n",
    "注意：在直方图中x轴代表了乘客的数量。\n",
    "\n",
    "**观察结果**\n",
    "\n",
    "- 年龄<4的生魂率非常高。\n",
    "\n",
    "- 年龄最大（80岁）的乘客活了下来。\n",
    "\n",
    "- 在15-25岁之间的有很多人没有活下来。\n",
    "\n",
    "- 大部分乘客都是在15-35岁之间。\n",
    "\n",
    "**决定**\n",
    "\n",
    "这个简单的分析确定了我们之前的假设。\n",
    "\n",
    "- 我们会把年龄放在我们的训练模型中（我们在**分类**中的 #2）\n",
    "\n",
    "- 在Age特征的空值填上特征（**填充** #1）\n",
    "\n",
    "- 我们应该给年龄分层（创造 #3）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train_df,col = 'Survived')\n",
    "g.map(plt.hist,'Age',bins = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数值特征与序列特征之间的相关性\n",
    "\n",
    "我们能使用一个图标来表达多个特征之间的相关性，因为数字特征和类别特征都是数值所以才可以这样用。\n",
    "\n",
    "**观察结果**\n",
    "\n",
    "- Pclass = 3有很多乘客，但他们大多数都没有活下来。这就代表我们的**分类**假设 #2 是正确的。\n",
    "\n",
    "- 在Pclass = 2与Pclass = 3的很多婴儿都活了下来，这进一步验证了**分类**假设 #2。\n",
    "\n",
    "- 大部分Pclass = 1的乘客都活了下来。这就代表我们的**分类**假设 #3 是正确的。\n",
    "\n",
    "- 在乘客的年龄分层方面Pclass个不相同。\n",
    "\n",
    "**决定**\n",
    "\n",
    "- Pclass作为我们的训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df,col = 'Survived',row = 'Pclass',size = 2.2,aspect = 1.6)\n",
    "grid.map(plt.hist,'Age',alpha = 0.5,bins = 20)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 类别特征的相关性\n",
    "\n",
    "现在我们要验证类别特征之间的相关性。\n",
    "\n",
    "**观察结果**\n",
    "\n",
    "- 女性乘客的存活率高于男性。验证了**分类** #1\n",
    "\n",
    "- 除了Embarked = C男性的生还率特别高之外，这可能是Pclass和Embarked之间的相关性，而不是Pclass和Survived之间的直接相关性。\n",
    "\n",
    "- 在C和Q港口上船的男性，Pclass = 3 比Pclass = 2的生还率还要高。（**完成** #2）\n",
    "\n",
    "**决定**\n",
    "\n",
    "- 在训练模型中添加Sex特征。\n",
    "\n",
    "- 在训练模型中添加并完成Embarked特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df,row = 'Embarked',size = 2.2,aspect = 1.6)\n",
    "grid.map(sns.pointplot,'Pclass','Survived','Sex',palette = 'deep')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 类别特征与数字特征之间的相关性\n",
    "\n",
    "我们也想关联类别特征和数字特征。我们现在关联Embarked（无数字类别），Sex（无数字类别），Fare（连续型数值）与Survived之间的关联。\n",
    "\n",
    "**观察结果**\n",
    "\n",
    "- 票价越高生存率也就越高。\n",
    "\n",
    "- 上车地点也与生存率有关。\n",
    "\n",
    "**决定**\n",
    "\n",
    "- 将Fare特征进行分层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df,row = 'Embarked', col = 'Survived', size = 2.2,aspect = 1.6)\n",
    "grid.map(sns.barplot,'Sex','Fare', alpha = 0.5,ci = None)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整理特征\n",
    "\n",
    "我们已经收集了关于我们的数据集和解决方案需求的几个假设和决定。到目前为止我我们不能改变单一的特征或者值。现在要根据我们的假设和决定来进行创造和填充特征。\n",
    "\n",
    "#### 删除无用的特征\n",
    "\n",
    "这是一个很好的开始执行目标，删除无用特征会让我们处理的数据点更少，加快我们的运行并简化分析。\n",
    "\n",
    "基于我们的假设和判断我们现在要删除Cabin和Ticket两个特征。\n",
    "\n",
    "在适用的情况下，我们一起对训练和测试数据集执行操作，以保持一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before',train_df.shape,test_df.shape,combine[0].shape,combine[1].shape)\n",
    "train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "combine = [train_df,test_df]\n",
    "\n",
    "'After ',train_df.shape,test_df.shape,combine[0].shape,combine[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在现有的基础上创造新的特征\n",
    "\n",
    "我们想要分析Name是否可以提取测试和生存之间的关联，然后再删除Name和PassengerId功能。\n",
    "\n",
    "在下面的代码中我们使用正则表达式提取特征。\n",
    "\n",
    "**观察结果**\n",
    "\n",
    "把Title，Age，和Survived用图标展示出来，能得到以下观察结果。\n",
    "\n",
    "- 大多数头衔都是准确地描述年龄的，例如：Master头衔的平均年龄为5岁。\n",
    "\n",
    "- 各个头衔之间的生存率略有不同。\n",
    "\n",
    "- 某些头衔的人大部分都活了下来，有一些没有。\n",
    "\n",
    "**决定**\n",
    "\n",
    "- 我们决定保留Tittle特征在训练模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "pd.crosstab(train_df['Title'], train_df['Sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以用一个更常见的名称替换许多标题或者将它们分类为Rare。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以把类别头衔转化为序列特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_maping = {\"Mr\":1,\"Miss\":2,\"Mrs\":3,\"master\":4,\"Rare\":5}\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].map(title_maping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们能在训练集和测试集上安全的删除Name特征。在训练集上我们也不需要PassengerId特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\n",
    "test_df = test_df.drop(['Name'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转换类别特征\n",
    "\n",
    "现在我们可以把特征从字符串转换为数值特征，因为大部分的模型算法都是需要数值特征。\n",
    "\n",
    "我们先把Sex特征（female = 1，male = 0）转化为新的Gender特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Sex'] = dataset['Sex'].map({\"female\":1,\"male\":0}).astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 填充连续型数值特征\n",
    "\n",
    "现在我们应该对缺失值或空白值进行评估和填充。我们首先对Age特征动手。\n",
    "\n",
    "我们这有三个方法来填充连续型特征。\n",
    "\n",
    "- 1.一个简答的方法是在平均值的标准差上下进行随机选择。\n",
    "\n",
    "- 2.猜测缺失值的更准确的方法是使用其他相关特征。在我们的案例中，我们注意到年龄，性别和Pclass之间的相关性。使用Pclass和Gender特征组合中Age的中位数值猜测年龄值。因此，Pclass = 1和Gender = 0的中位数年龄，Pclass = 1和Gender = 1等等\n",
    "\n",
    "- 3.结合方法1与方法2.\n",
    "\n",
    "方法1和3会引入随机噪声，每次执行获得的结果可能都不相同。所以我们选择方法2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df,row ='Pclass',col = 'Sex',size = 2.2,aspect = 1.6)\n",
    "grid.map(plt.hist,'Age',alpha = .5,bins = 20)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备一个空数组，以包含基于Pclass和Gender组合的猜测年龄值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_ages = np.zeros((2,3))\n",
    "guess_ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们遍历Sex（0或1）和Pclass（1，2，3）来计算6种组合的Age的猜测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    for i in range(0,2):\n",
    "        for j in range(0,3):\n",
    "            guess_df = dataset[(dataset['Sex'] == i)&\\\n",
    "                              (dataset['Pclass'] ==j+1)]['Age'].dropna()\n",
    "            \n",
    "            age_guess = guess_df.median()\n",
    "            \n",
    "            guess_ages[i,j] = int(age_guess/0.5+0.5)*0.5\n",
    "            \n",
    "        \n",
    "    for i in range(0,2):\n",
    "        for j in range(0,3):\n",
    "            dataset.loc[(dataset.Age.isnull())&(dataset.Sex == i)&(dataset.Pclass == j+1),\\\n",
    "                       'Age'] = guess_ages[i,j]\n",
    "            \n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们对年龄进行分层，并确定他们与Survived之间的相关性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['AgeBand'] = pd.cut(train_df['Age'],5)\n",
    "train_df[['AgeBand','Survived']].groupby(['AgeBand'],as_index=False).mean().sort_values(by='AgeBand',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将年龄的取值范围转换为序数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:    \n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们不用移除AgeBand特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['AgeBand'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从现有的特征来创造新的特征\n",
    "\n",
    "我们现在创在一个新的特征FamilySize基于Parch和SibSp。这会让我们把Parch和SibSp从我们的数据集上删除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['FamilySize'] = dataset['SibSp']+dataset['Parch']+1\n",
    "    \n",
    "train_df[['FamilySize','Survived']].groupby(['FamilySize'],as_index=False).mean().sort_values(by='Survived',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们创造另一个特征IsAlone。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1 , 'IsAlone'] = 1\n",
    "\n",
    "train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除Parch，SibSp和FamilySize特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
    "test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在创造一个人工特征将Pclass和Age结合起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n",
    "\n",
    "train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 填充类别特征\n",
    "\n",
    "Embarked特征有S,Q,C三个值。我们的训练集有两个缺失值，所以我们填充两个最常见的特征值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_port = train_df.Embarked.dropna().mode()[0]\n",
    "freq_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n",
    "    \n",
    "train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将类别特征转换为数值特征\n",
    "\n",
    "想在将Embaredde字符串特征转化为数值特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map({'S':0,'C':1,'Q':2}).astype(int)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 快速填充和转化数值特征\n",
    "\n",
    "我们现在可以在测试集上选择最常出现的值作为单个缺失值填充Fare特，我们在一行代码中执行此操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创造一个新的FareBand特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['FareBand'] = pd.qcut(train_df['Fare'],4)\n",
    "train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将Fare特征转换为序列值特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "\n",
    "train_df = train_df.drop(['FareBand'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "    \n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型、预测和解决问题\n",
    "\n",
    "现在我们准备好训练模型和预测所需的解决方案。有60多种建模算法可供选择。我们必须了解问题类型和解决方案要求，以便缩小到我们可以评估的少数几个模型。我们的问题是分类和回归问题。我们希望确定输出（生存与否）与其他变量或功能（性别，年龄，端口...）之间的关系。我们也正在执行一类机器学习，因为我们正在使用给定的数据集来训练我们的模型，所以称为监督学习。有了这两个标准 - 监督学习是回归，我们可以将我们的模型选择缩小到几个。这些包括：\n",
    "\n",
    "- Logistic Regression\n",
    "- KNN or k-Nearest Neighbors\n",
    "- Support Vector Machines\n",
    "- Naive Bayes classifier\n",
    "- Decision Tree\n",
    "- Random Forrest\n",
    "- Perceptron\n",
    "- Artificial neural network\n",
    "- RVM or Relevance Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop(\"Survived\",axis =1 )\n",
    "y_train = train_df['Survived']\n",
    "x_test = test_df.drop('PassengerId',axis = 1).copy()\n",
    "x_train.shape,y_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression在我们的工作早期运行是非常有必要的。逻辑回归通过使用逻辑函数估计概率来测量分类因变量（特征）和一个或多个自变量（特征）之间的关系，该逻辑函数是累积逻辑分布。 参考维基百科。\n",
    "\n",
    "请注意我们训练数据集生成的预测结果、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train,y_train)\n",
    "y_pred = logreg.predict(x_test)\n",
    "acc_log = round(logreg.score(x_train,y_train)*100,2)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用Logistic回归来验证我们对功能创建和完成目标的假设和决策。 这可以通过计算决策函数中的特征的系数来完成。\n",
    "\n",
    "正系数增加了响应的对数几率（从而增加了概率），负系数降低了响应的对数几率（从而降低了概率）。\n",
    "\n",
    "- 性别是最高的正系数，意味着性别对（男性：0到女性：1）生存率= 1的概率增加最多。\n",
    "- 相反Pclass升高与Survived = 1的增加的最大。\n",
    "- 这样Age * Class是一个很好的人造模型，因为它与Survived具有次高的负相关性。\n",
    "- 标题也是第二高的正相关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coff_df = pd.DataFrame(train_df.columns.delete(0))\n",
    "coff_df.columns=['Feature']\n",
    "coff_df['Correlation']= pd.Series(logreg.coef_[0])\n",
    "\n",
    "coff_df.sort_values(by='Correlation', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们使用支持向量机进行建模，支持向量机是具有关联学习算法的监督学习模型，学习算法分析用于分类和回归分析的数据。给定一组训练样本，每组标记为属于两个类别中的一个或另一个，SVM训练算法建立一个模型，将新的测试样本分配给一个类别或另一个类别，从而使其成为非概率二元线性分类器。 参考维基百科。\n",
    "\n",
    "请注意，该模型生成的分数高于逻辑回归模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(x_train,y_train)\n",
    "Y_pred = svc.predict(x_test)\n",
    "acc_svc=round(svc.score(x_train,y_train)*100,2)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模式识别中，k-最近邻居算法（简称k-NN）是一种用于分类和回归的非参数方法。 一个样本通过相邻的多数投票进行分类，样本被分配到k个最近邻中最常见的类别（k是一个正整数，通常很小）。 如果k = 1，则将该对象简单地分配给该单个最近的类别。 参考维基百科。\n",
    "\n",
    "KNN置信度比逻辑回归和SVM好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_train,y_train)\n",
    "Y_pred= knn.predict(x_test)\n",
    "acc_knn=round(knn.score(x_train,y_train)*100,2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习中，朴素贝叶斯分类器是一个简单的概率分类器，它基于贝叶斯定理和特征之间的独立性假设。朴素贝叶斯分类器具有高度可扩展性，在学习问题中需要许多变量（特征）的线性参数。 参考维基百科。\n",
    "\n",
    "模型生成的置信度是目前评估的模型中最低的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = GaussianNB()\n",
    "gaussian.fit(x_train, y_train)\n",
    "Y_pred = gaussian.predict(x_test)\n",
    "acc_gaussian = round(gaussian.score(x_train, y_train) * 100, 2)\n",
    "acc_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知器是用于二元分类器的监督学习的算法（可以决定输入由数字向量表示的属于某个特定类别的函数）。 它是一种线性分类器，即一种分类算法，它基于将一组权重与特征向量相结合的线性预测函数进行预测。 该算法允许在线学习，因为它一次处理训练集中的元素。 参考维基百科。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(x_train, y_train)\n",
    "Y_pred = perceptron.predict(x_test)\n",
    "acc_perceptron = round(perceptron.score(x_train, y_train) * 100, 2)\n",
    "acc_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(x_train, y_train)\n",
    "Y_pred = linear_svc.predict(x_test)\n",
    "acc_linear_svc = round(linear_svc.score(x_train, y_train) * 100, 2)\n",
    "acc_linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.fit(x_train, y_train)\n",
    "Y_pred = sgd.predict(x_test)\n",
    "acc_sgd = round(sgd.score(x_train, y_train) * 100, 2)\n",
    "acc_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此步使用决策树作为预测模型，将特征（树分支）映射为关于目标值（树叶）的结论。 目标变量可以采用有限的一组值的树模型称为分类树; 在这些树结构中，叶代表类标签，分支代表导致这些类标签的功能的连接。变量可以采用连续值（通常是实数）的决策树被称为回归树。 参考维基百科。\n",
    "\n",
    "迄今为止评估的模型中，模型置信度得分最高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(x_train, y_train)\n",
    "Y_pred = decision_tree.predict(x_test)\n",
    "acc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林是最受欢迎的模型之一。随机森林或随机决策森林是一种用于分类，回归和其他任务的集合学习方法，它通过在训练时构造大量决策树（n_estimators = 100）并输出作为类的模式的分类 或预测回归单个树。参考维基百科。\n",
    "\n",
    "迄今为止评估的模型中，模型置信度得分最高。 我们决定使用这个模型的输出（Y_pred）来创建竞赛结果提交。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(x_train, y_train)\n",
    "Y_pred = random_forest.predict(x_test)\n",
    "random_forest.score(x_train, y_train)\n",
    "acc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \n",
    "              'Decision Tree'],\n",
    "    'Score': [acc_svc, acc_knn, acc_log, \n",
    "              acc_random_forest, acc_gaussian, acc_perceptron, \n",
    "              acc_sgd, acc_linear_svc, acc_decision_tree]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "我们提交给比赛网站Kaggle的结果是在6,082项参赛作品中得到3,883分。\n",
    "\n",
    "**参考**\n",
    "这个notebook是基于解决泰坦尼克号比赛和其他资源创建的。\n",
    "\n",
    "- 通过泰坦尼克号的旅程  https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic\n",
    "\n",
    "- pandas入门：Kaggle的泰坦尼克号比赛  https://www.kaggle.com/c/titanic\n",
    "\n",
    "- 泰坦尼克号最佳工作分类器  https://www.kaggle.com/sinakhorami/titanic-best-working-classifier?scriptVersionId=566580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
