#  第2章 模型评估与选择

## 2.1 经验误差与过拟合

**错误率（error rate）：**分类错误的样本数占样本总数的比例E =a/m

**精度（accuracy）：**1-a/m。即1-错误率

**误差（error）：**学习器的实际预测输出与样本的真是输出之间的差异。

**训练误差（training error）或经验误差（empirical error）：**学习器在训练集上的误差。

**泛化误差：**在新样本上的误差

**过拟合（overfitting）：**学习器把训练样本学的太好，很可能吧训练样本自身的特点当做所有潜在样本都具有的一般性质，导致繁华性能下降。

**欠拟合（underfitting）：**对训练样本的一般性质尚未学习好。

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180420/E3iBbbj46b.png)

## 2.2 评估方法

可以通过是按测试来对学习器的泛化误差进行评估并做出选择。

### 2.2.1 留出法

**留出法（hold-out）：**直接将数据集D划分为两个互斥的集合，其中一个作为训练集，另一个作测试集。（在数据划分时要尽可能的保持数据分布的一致性，避免因数据划分而影响结果。）

### 2.2.2 交叉验证法

**交叉验证法（cross validation）：**先将数据集D划分为k个大小相似的互斥子集，每个子集$D_{i}$都尽可能保持数据分布的一致性。然后，每次用$k-1$个子集的并集作为训练集，余下的那个子集作为训练集；这样就可获得$k$组训练/测试集，从而进行$k$次训练和测试，最终返回的是$k$个测试结果的均值。

### 2.2.3 自助法

**自助法（bootstrapping）：**它直接以自动采样的方法为基础。给定包含$m$个样本的数据集$D$，我们对它进行采样产生数据集$D^{'}$：每次随即从$D$中挑选一个样本，将其拷贝放入$D^{'}$中，然后在将样本放回$D$中，使得它下次仍能被采到；重复$m$次。（通过自助采样，初始数据集中仍有36.8%为出现在训练集中）（在数据集较小，难以有效划分训练/测试集时很有用）

### 2.2.4 调参与最终模型

**验证集（validation set）：**模型评估与选择中用于评估测试的数据集。

## 2.3 性能度量

**性能度量（performance measure）：**对学习器的泛化能力进行评估，不仅需要有效可行的实验评估方法，还需要衡量模型泛化能力的评价标准。

回归任务中最常用的性能度量是**均方误差（mean squared error）**

$$
E(f;D) = \frac{1}{m}\sum_{i = 1}^{m}(f(x_{i})-y_{i})^{2}
$$

更一般的，对于数据分布$D$和概率密度函数$p(.)$，均方误差可描述为

$$
E(f;D) = \int_{\infty ~ D}(f(x_{i})-y_{i})^{2}p(x)dx
$$

### 2.3.1 错误率与精度

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180420/bEC1k1Lb51.png)

查准率P:

$$
P = \frac{TP}{TP+FP}
$$

查全率R

$$
R = \frac{TP}{TP+FN}
$$

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180420/mjB8lF24Fe.png)

**F1度量：**

$$
F1 = \frac{{R}\times{2}{\times{P}}}{P+R}=\frac{{x}\times{TP}}{样本总数+TP-TN}
$$
依据：
$$
\frac{1}{F1} = \frac{1}{2}\times{(\frac{1}{P}+\frac{1}{R})}
$$

在某些应用中对查准率和查全率的重视程度有所不同，假设查全率更重要，那F1度量的一般形式——$F_{\beta}$，能让我们表达出对查准率/查全率的不同偏好，它定义为：

$$
F_{\beta} = \frac{(1+\beta_{2}\times P\times R)}{(\beta_{2}\times P) + R}
$$

$\beta$ 代表了查全率的重要性。

###　2.3.3　ＲＯＣ与ＡＵＣ

很多学习器是为预测样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。

ＲＯＣ全称是“受试者工作特征”曲线

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180420/hfHfgbcDA4.png)

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180420/G9aCCbB2Ck.png)

ＲＯＣ曲线下的面积为ＡＵＣ。

假定ＲＯＣ曲线由坐标　{$(x_{1},y_{1}),(x_{2},y_{2}),......,(x_{n},y_{n})$}的点按顺序连接而形成，则AUC可估算为：
$$
AUC = \frac{1}{2}\sum_{i = 1}^{m-1}(x_{i+1}-x_{i})\times (y_{i}=y_{i+1})
$$

### 2.3.4 代价敏感错误率与代价曲线

**非均等代价价（unequal cost）：**不同类型的错误所造成的后果不同，为权衡不同类型错误所造成的不同损失。

## 2.4 比较检验

比较检验涉及到几个重要要素：

- 我们希望比较的是泛化性能，然后通过实验评估方法我们获得的是测试集上的性能，两者对比的结果可能未必相同。

- 测试集上的性能与测试集本身的选择有很大关系，且不论使用不同大小的测试集会得到不同的结果，即使使用大小相同的测试集，若包含的样例不同，测试结果也可能不同。

- 很多机器学习算法本身具有一定的随机性，即便用相同的参数设置在同一个测试集上多次运行，其结果也会有不同。

那么：

**统计假设检验（hypothesis test）**为我们进行学习器性能比较提供了重要依据。 （本节默认以错误率为性能度量，用$\varepsilon$表示）

### 2.4.1 假设检验

















