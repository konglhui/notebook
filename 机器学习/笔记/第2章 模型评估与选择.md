#  第2章 模型评估与选择

## 2.1 经验误差与过拟合

**错误率（error rate）：**分类错误的样本数占样本总数的比例E =a/m

**精度（accuracy）：**1-a/m。即1-错误率

**误差（error）：**学习器的实际预测输出与样本的真是输出之间的差异。

**训练误差（training error）或经验误差（empirical error）：**学习器在训练集上的误差。

**泛化误差：**在新样本上的误差

**过拟合（overfitting）：**学习器把训练样本学的太好，很可能吧训练样本自身的特点当做所有潜在样本都具有的一般性质，导致繁华性能下降。

**欠拟合（underfitting）：**对训练样本的一般性质尚未学习好。

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180420/E3iBbbj46b.png)

## 2.2 评估方法

可以通过是按测试来对学习器的泛化误差进行评估并做出选择。

### 2.2.1 留出法

**留出法（hold-out）：**直接将数据集D划分为两个互斥的集合，其中一个作为训练集，另一个作测试集。（在数据划分时要尽可能的保持数据分布的一致性，避免因数据划分而影响结果。）

### 2.2.2 交叉验证法

**交叉验证法（cross validation）：**先将数据集D划分为k个大小相似的互斥子集，每个子集$D_{i}$都尽可能保持数据分布的一致性。然后，每次用$k-1$个子集的并集作为训练集，余下的那个子集作为训练集；这样就可获得$k$组训练/测试集，从而进行$k$次训练和测试，最终返回的是$k$个测试结果的均值。

### 2.2.3 自助法

**自助法（bootstrapping）：**它直接以自动采样的方法为基础。给定包含$m$个样本的数据集$D$，我们对它进行采样产生数据集$D^{'}$：每次随即从$D$中挑选一个样本，将其拷贝放入$D^{'}$中，然后在将样本放回$D$中，使得它下次仍能被采到；重复$m$次。（通过自助采样，初始数据集中仍有36.8%为出现在训练集中）（在数据集较小，难以有效划分训练/测试集时很有用）

### 2.2.4 调参与最终模型

**验证集（validation set）：**模型评估与选择中用于评估测试的数据集。

## 2.3 性能度量

**性能度量（performance measure）：**对学习器的泛化能力进行评估，不仅需要有效可行的实验评估方法，还需要衡量模型泛化能力的评价标准。























