# 第3章 线性模型

## 3.1 基本形式

**线性模型：**试图学得一个通过属性的线性组合来进行预测的函数。即：

$$
f(x) =w_{1}x_{1}+w_{2}x_{2}+...+w_{n}x_{n}+b
$$

$$
f(x) = w^{T}x+b
$$

## 3.2 线性回归

**线性回归：**试图学得一个线性模型以尽可能准确地预测实值输出标记。

线性回归试图学得：

$$
f(x_{i}) = w^{T}x_{i}+b,使得f(x_{i}) = y_{i}
$$

如何确定$w和b，关键就是f(x) 与y$。在2.3节中介绍过均方误差，其是回归任务中最常用的性能度量，因此我们可以试图让军方误差最小化。

$$
(w^{*},b^{*}) = arg \min \sum_{i=1}^{m}(f(x_{i})-y_{i})^{2}
$$

在$\sum_{i=1}^{m}(f(x_{i})-y_{i})^{2}$中我们对$w,b$分别求导，可得$w,b$的最优解：

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180426/4JHCJ4m1mm.png)

**多元线性回归：**样本中，样本由d个属性描述。即$y_{i}$不止两个值。

类似的可以使用最小二乘法对$w,b$进行估计，具体见书55页。

## 3.3 对数几率回归

考虑而分类任务，其输出标记$y \in$ {0,1}，而线性回归模型产生的预测值$z = w^{T} +b$是实值，于是，我们需将实值z转换为0/1值。

## 3.4 线性判别分析

线性判别分析（linear discriminat analysis 简称LDA）是一种经典的线性学习方法。

LDA的思想非常朴素：给性训练样例集，设法将样例投影到一条直线上，是的同样样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，在根据投影点的位置确定新样本的类别。图例如下：

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180502/aGkElh4ja8.png)



## 3.5 多分类学习

在很多情况下，我们是基于一些基本策略，利用二分类学习器来解决多分类问题。

最经典的拆分策略有三中：“一对一”，"一对其余"，“多对多”。

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180503/EE1lici02b.png)

## 3.6 类别不平衡问题

如果不同类别的训练样例数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰。

类别不平衡就是指：分类任务中不同类别的训练样例数目差别很大时，不失一般性。

事实上是在用预测出的y值与阈值进行比较，例如通常在y>0.5时判别为正例，否则为反例，y实际上表达了正例的可能性，几率$\frac{y}{1-y}$则反映了正例可能性与反例可能性的比值。阈值为0.5表明分类器认为真实正反例可能性相同，即分类器决策规律为若$\frac{y}{1-y} > 1$则预测为正例。

然而当训练集中正反例数目不同时，另$m^{+}$表示正例数目，$m^{-}$表示反例数目，则观测几率时$\frac{m^{+}}{m^{-}}$,由于我们通常假设训练集时真实样本总体的无偏采样，因测观测几率就是代表了真实几率，于是，只要分类器的预测几率高于观测几率就应判定为正例，即：

$$
\frac{y}{1-y}>\frac{m^{+}}{m^{-}} 则，预测为正例
$$


















