# 第4章 决策树

## 4.1 基本流程

决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：

- （1）当前借点包含的样本全属于同一类别，无需划分；
- （2）当前属性集为空，或是样本在所有属性上取值相同，无法划分；
- （3）当前节点包含的样本集合为空，不能划分。

在（2）情形下，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别；在第（3）中情况下，同样把当前结点标记为叶节点，但将其类别设定为其父结点所含样本最多的类别。注意这两种情形的处理实质不同；情形（2）是在利用当前结点的后验分布，而情形（3）则是把父结点的样本分布作为当前结点的先验分布。

## 4.2 划分选择

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180508/KGdE9ff2B3.png)

由上图可以看出，决策树最关键是第8行，即如何选择最优划分属性。我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即结点的纯度（purity）越来越高。

### 4.2.1 信息增益

“信息熵”（information entropy）是度量样本集合纯度最常用的一种指标。假定当前样本集合D中第k类样本所占的比例为$p_{k}(k = 1,2,3,......|y|)$,则D的信息熵定义为：
$$
Ent(D) = -\sum^{|y|}_{k =1}p_{k}log_{2}p_{k}
$$
Ent(D)的值越小，则D的纯度越高。

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180508/LdJjDc60JH.png)
$$
Gain(D,a) = Ent(D) - \sum_{v = 1}^{V}\frac{|D^{V}|}{|D|}Ent(D^{v})
$$
一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度”提升越大。因此我们可以用信息增益来进行决策树的划分属性选择。这就是ID3算法。

案例：见书76页。

### 4.2.2 增益率

信息增益准则对可取数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响

，不直接使用信息增益，而是使用“增益率”来选择最优划分属性。（C4.5算法）增益率定义为：
$$
Gain_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}
$$
其中：
$$
IV(a) = -\sum_{v = 1}^{V}\frac{|D^{v}|}{|D|}log_{2}\frac{|D_{v}|}{|D|}
$$
需注意的是，增益率准则对可取值数目较少的属性有所偏好，因此，C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式；先从候选划分属性中找出信息增益高于平均水平的属性，在从中找出增益率最高的。

### 4.2.3 基尼指数

CART决策树使用“基尼指数”来选择划分属性。采用与式（4.1）相同的符号，数据集D的纯度可用基尼值来度量：
$$
Gini(D) = \sum_{k=1}^{|y|}\sum_{k^{`}\neq k}p_{k}p_{k^{`}} = 1-\sum_{k=1}^{|y|}p_{k^{`}}^{2}
$$
直观来说，$Gini(D)$反映了数据集中D中随机抽取两个样本，其类别标记不一致的概率，因此，$Gini(D)$越小，则数据集的纯度越高。

属性a饿基尼指数定义为：

$$
Gain\_index(D,a) = \sum_{v = 1}^{V}\frac{|D^{V}|}{|D|}Gini(D^{v})
$$

所以在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最有划分属性，

## 4.3 剪枝处理

剪枝是决策树学习算法对付“过拟合”的主要手段。

决策树剪枝的基本策略有“预剪枝”和“后剪枝”。

- 预剪枝是在决策树生成过程中，对每个结点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；
- 后剪枝是先从训练集生成一刻完整的决策树，然后自底向上对非叶节点进行考察，若该结点对应的子树替换为叶结点能带来决策是泛化性能提升，则将子树替换为叶结点。

如何判断决策树泛化性能是否提升，可以使用2.2节的性能评估方法。题解使用留出法。

### 4.3.1 预剪枝

具体案例见书82页。

预剪枝基于“贪心”本质禁止分支展开，这给预剪枝决策树带来了欠拟合的风险。

### 4.3.2 后剪枝  

具体案例见书82页。

后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般后剪枝的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝是在生成完全决策树之后进行的，并且要自底向上对所有非叶结点逐一考察，因此训练开销更大。

## 4.4 连续与缺失值

### 4.4.1 连续值处理

最简单的策略是采用二分法对连续属性进行处理，着正是C4.5决策树算法中才用的机制。

### 4.4.2 缺失值处理

**我们需要解决两个问题：**

- 如何在属性值缺失的情况下进行划分属性选择？
- 给定划分属性，若样本在该属性上的值确实，如何对样本进行划分？

给定数据集D和属性a，令$D^{'}$表示D中在属性a上没有的缺失值的样本子集。对问题（1），显然我们仅可根据$D^{'}$来判断属性a的优劣。假定属性a有V个可取值{$a^{1},a^{2},...,a^{V}$}令$D^{V'}$表示$D^{'}$中属性a上$a^{v}$的样本子集，$D^{'}_{k}$表示$D^{'}$中属于第k类（k= 1,2,,...,|y|）的样本子集，显然$D^{'} = \sum^{|y|}_{k =1}D^{'}_{k}$,$D^{'} = \sum^{V}_{v =1}D^{V'}$.假设沃恩为每个样本x赋予一个权重w，并定义：

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180509/m2blaK4Bk0.png)

直观的看，对属性a，p表示无缺失值样本所占的比例，$p^{'}_{k}$表示无缺失值样本中第k类所占的比例，$r^{'}_{v}$则表示无缺失值样本中在属性a上取值$a_{v}$的样本所占的比例。$\sum^{|y|}_{k =1}p_{k} = 1$$ \sum^{V}_{v =1}r_{V'} = 1$.

基于上述定义，我们可将细腻增益的计算式（4.2）推广为：

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180509/kGKG4aFJmB.png)

对问题（2），若样本x在划分属性a上的取值已知，则将x划入与其取值对应的子节点，且忘本权值在子节点中保持为$w_{x}$.若样本x在划分属性a上的去hi未知，则将x同时划入所有子结点，且样本权值在与属性值对应的子节点中调整$r^{'}_{v}\times w_{x}$;直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去。

##  4.5 多变量决策树

“单变量决策树”每一段都是与坐标轴平行的。这样的分类结果使得学习结果有较好的可解释性。但此时的决策树或相当复杂，由于要进行大量的属性测试，预测时间开销会很大。

“多变量决策树”能实现“斜划分”甚至是更复杂的决策树。在此类决策树中，非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试。所以与“单变量决策树”不同的是，它是试图建立一个适合的线性分类器。

