# 第4章 决策树

## 4.1 基本流程

决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：

- （1）当前借点包含的样本全属于同一类别，无需划分；
- （2）当前属性集为空，或是样本在所有属性上取值相同，无法划分；
- （3）当前节点包含的样本集合为空，不能划分。

在（2）情形下，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别；在第（3）中情况下，同样把当前结点标记为叶节点，但将其类别设定为其父结点所含样本最多的类别。注意这两种情形的处理实质不同；情形（2）是在利用当前结点的后验分布，而情形（3）则是把父结点的样本分布作为当前结点的先验分布。

## 4.2 划分选择

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180508/KGdE9ff2B3.png)

由上图可以看出，决策树最关键是第8行，即如何选择最优划分属性。我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即结点的纯度（purity）越来越高。

### 4.2.1 信息增益

“信息熵”（information entropy）是度量样本集合纯度最常用的一种指标。假定当前样本集合D中第k类样本所占的比例为$p_{k}(k = 1,2,3,......|y|)$,则D的信息熵定义为：
$$
Ent(D) = -\sum^{|y|}_{k =1}p_{k}log_{2}p_{k}
$$
Ent(D)的值越小，则D的纯度越高。

![mark](http://p6yio0wew.bkt.clouddn.com/blog/180508/LdJjDc60JH.png)
$$
Gain(D,a) = Ent(D) - \sum_{v = 1}^{V}\frac{|D^{V}|}{|D|}Ent(D^{v})
$$
一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度”提升越大。因此我们可以用信息增益来进行决策树的划分属性选择。这就是ID3算法。

案例：见书76页。

### 4.2.2 增益率

信息增益准则对可取数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响

，不直接使用信息增益，而是使用“增益率”来选择最优划分属性。（C4.5算法）增益率定义为：
$$
Gain_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}
$$
其中：
$$
IV(a) = -\sum_{v = 1}^{V}\frac{|D^{v}|}{|D|}log_{2}\frac{|D_{v}|}{|D|}
$$
需注意的是，增益率准则对可取值数目较少的属性有所偏好，因此，C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式；先从候选划分属性中找出信息增益高于平均水平的属性，在从中找出增益率最高的。

### 4.2.3 基尼指数

CART决策树使用“基尼指数”来选择划分属性。采用与式（4.1）相同的符号，数据集D的纯度可用基尼值来度量：
$$
Gini(D) = \sum_{k=1}^{|y|}\sum_{k^{`}\neq k}p_{k}p_{k^{`}} = 1-\sum_{k=1}^{|y|}p_{k^{`}}^{2}
$$
直观来说，$Gini(D)$反映了数据集中D中随机抽取两个样本，其类别标记不一致的概率，因此，$Gini(D)$越小，则数据集的纯度越高。

属性a饿基尼指数定义为：

$$
Gain\_index(D,a) = \sum_{v = 1}^{V}\frac{|D^{V}|}{|D|}Gini(D^{v})
$$

所以在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最有划分属性，

















